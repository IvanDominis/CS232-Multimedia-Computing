{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoàng Đình Hữu - MSSV: 20521384"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEVENSHTEIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAMMING DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming(s1, s2):\n",
    "    i ,count = 0, 0\n",
    "    while(i < len(s1)):\n",
    "        if(s1[i] != s2[i]): count += 1\n",
    "        i += 1\n",
    "    return count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JACCARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(x,y):\n",
    "  intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "  union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "  return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "0.6666666666666666\n",
      "0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "s1 = 'Bottle'\n",
    "s2 = 'Beatle'\n",
    "for i in [levenshtein,hamming,jaccard]:\n",
    "    print(i(s1,s2))\n",
    "sentences = [\"The bottle is empty\",\"There is nothing in the bottle\"]\n",
    "s = [sent.lower().split(\" \") for sent in sentences]\n",
    "print(jaccard(s[0],s[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "def unique(sequence):\n",
    "    word = set()\n",
    "    return [x for x in sequence if not (x in word or word.add(x))]\n",
    "def Preprocess(s1,s2):\n",
    "    s1 = s1.lower()\n",
    "    s2 = s2.lower()\n",
    "    tokens1 = s1.split()\n",
    "    tokens2 = s2.split()\n",
    "    vocab = unique(tokens1+tokens2)\n",
    "    print(tokens1,tokens2)\n",
    "    print(vocab,len(vocab))\n",
    "    filtered = []\n",
    "    for i in vocab: \n",
    "        # if i not in set(stopwords.words('english')):\n",
    "            filtered.append(i)\n",
    "    return tokens1,tokens2,filtered\n",
    "def vectorize(tokens,filtered):\n",
    "    vector=[]\n",
    "    for i in filtered:\n",
    "        vector.append(tokens.count(i))\n",
    "    print(vector)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'will', 'go', 'out', 'with', 'her', 'tonight'] ['she', 'hope', 'that', 'i', 'will', 'give', 'her', 'a', 'present', 'tonight']\n",
      "['i', 'will', 'go', 'out', 'with', 'her', 'tonight', 'she', 'hope', 'that', 'give', 'a', 'present'] 13\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "a = 'I will go out with her tonight'\n",
    "b = 'She hope that I will give her a present tonight'\n",
    "t1,t2,filtered = Preprocess(a,b)\n",
    "v1 = vectorize(t1,filtered)\n",
    "v2 = vectorize(t2,filtered)\n",
    "# print('Word Similarity using Levenshtein:',levenshtein(v1,v2))\n",
    "# print('Word Similarity using Hamming Distance:',hamming(v1,v2))\n",
    "# print('Word Similarity using Jaccard:',jaccard(v1,v2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return (tfDict)\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "    return(idfDict)\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return (tfidf)\n",
    "def Preprocess_tf_idf(a,b):\n",
    "    a = a.split()\n",
    "    b = b.split()\n",
    "    total= set(a).union(set(b))\n",
    "    print(total)\n",
    "    wordDictA = dict.fromkeys(total, 0) \n",
    "    wordDictB = dict.fromkeys(total, 0)\n",
    "    for word in a:\n",
    "        wordDictA[word]+=1  \n",
    "    for word in b:\n",
    "        wordDictB[word]+=1\n",
    "    print('',wordDictA,'\\n',wordDictB)\n",
    "    tfa = computeTF(wordDictA, a)\n",
    "    tfb = computeTF(wordDictB, b)\n",
    "    filtered_sentence = []\n",
    "    for word in wordDictA:\n",
    "        w = str(word)\n",
    "        filtered_sentence.append(w)\n",
    "    idfs = computeIDF([wordDictA, wordDictB])\n",
    "    idfa = computeTFIDF(tfa, idfs)\n",
    "    idfb = computeTFIDF(tfb, idfs)\n",
    "    return list(round(i,4) for i in idfa.values()),list(round(i,4) for i in idfb.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'her', 'go', 'out', 'I', 'with', 'will', 'that', 'present', 'give', 'hope', 'She', 'tonight', 'a'}\n",
      " {'her': 1, 'go': 1, 'out': 1, 'I': 1, 'with': 1, 'will': 1, 'that': 0, 'present': 0, 'give': 0, 'hope': 0, 'She': 0, 'tonight': 1, 'a': 0} \n",
      " {'her': 1, 'go': 0, 'out': 0, 'I': 1, 'with': 0, 'will': 1, 'that': 1, 'present': 1, 'give': 1, 'hope': 1, 'She': 1, 'tonight': 1, 'a': 1}\n",
      " [0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043, 0.0] \n",
      " [0.0301, 0.0, 0.0, 0.0301, 0.0, 0.0301, 0.0301, 0.0301, 0.0301, 0.0301, 0.0301, 0.0301, 0.0301]\n"
     ]
    }
   ],
   "source": [
    "a = 'I will go out with her tonight'\n",
    "b = 'She hope that I will give her a present tonight'\n",
    "v1,v2 = Preprocess_tf_idf(a,b)\n",
    "print('',v1,'\\n',v2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COSINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'bottle', 'is', 'empty'] ['there', 'is', 'nothing', 'in', 'the', 'bottle']\n",
      "['the', 'bottle', 'is', 'empty', 'there', 'nothing', 'in'] 7\n",
      "[1, 1, 1, 1, 0, 0, 0]\n",
      "[1, 1, 1, 0, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.612"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt, pow, exp\n",
    "def squared_sum(x):\n",
    "    return round(sqrt(sum([a*a for a in x])),3)\n",
    "def cos_similarity(x,y):\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = squared_sum(x)*squared_sum(y)\n",
    "    return round(numerator/float(denominator),3)\n",
    "\n",
    "a = \"The bottle is empty\"\n",
    "b = \"There is nothing in the bottle\"\n",
    "t1,t2,filtered = Preprocess(a,b)\n",
    "v1 = vectorize(t1,filtered)\n",
    "v2 = vectorize(t2,filtered)\n",
    "cos_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
